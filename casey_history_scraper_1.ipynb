{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "37910def",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-17T07:12:30.034671Z",
     "start_time": "2024-03-17T06:20:24.374456Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up the Selenium WebDriver...\n",
      "Navigating to the initial URL...\n",
      "Clicking the necessary button to get to the search page...\n",
      "Entering the desired date range...\n",
      "Clicking the search button...\n",
      "Starting to scrape data...\n",
      "Scraping data from page 1...\n",
      "Accessing details of item 1 on page 1...\n",
      "Data from item 1 scraped. Returning to the list...\n",
      "Accessing details of item 2 on page 1...\n",
      "Data from item 2 scraped. Returning to the list...\n",
      "Accessing details of item 3 on page 1...\n",
      "Data from item 3 scraped. Returning to the list...\n",
      "Accessing details of item 4 on page 1...\n",
      "Data from item 4 scraped. Returning to the list...\n",
      "Accessing details of item 5 on page 1...\n",
      "Data from item 5 scraped. Returning to the list...\n",
      "Accessing details of item 6 on page 1...\n",
      "Data from item 6 scraped. Returning to the list...\n",
      "Accessing details of item 7 on page 1...\n",
      "Data from item 7 scraped. Returning to the list...\n",
      "Accessing details of item 8 on page 1...\n",
      "Data from item 8 scraped. Returning to the list...\n",
      "Accessing details of item 9 on page 1...\n",
      "Data from item 9 scraped. Returning to the list...\n",
      "Accessing details of item 10 on page 1...\n",
      "Data from item 10 scraped. Returning to the list...\n",
      "Accessing details of item 11 on page 1...\n",
      "Data from item 11 scraped. Returning to the list...\n",
      "Accessing details of item 12 on page 1...\n",
      "Data from item 12 scraped. Returning to the list...\n",
      "Accessing details of item 13 on page 1...\n",
      "Data from item 13 scraped. Returning to the list...\n",
      "Accessing details of item 14 on page 1...\n",
      "Data from item 14 scraped. Returning to the list...\n",
      "Accessing details of item 15 on page 1...\n",
      "Data from item 15 scraped. Returning to the list...\n",
      "Navigating to page 2.\n",
      "Scraping data from page 2...\n",
      "Accessing details of item 1 on page 2...\n",
      "Data from item 1 scraped. Returning to the list...\n",
      "Successfully navigated to page 2.\n",
      "Accessing details of item 2 on page 2...\n",
      "Data from item 2 scraped. Returning to the list...\n",
      "Successfully navigated to page 2.\n",
      "Accessing details of item 3 on page 2...\n",
      "Data from item 3 scraped. Returning to the list...\n",
      "Successfully navigated to page 2.\n",
      "Accessing details of item 4 on page 2...\n",
      "Data from item 4 scraped. Returning to the list...\n",
      "Successfully navigated to page 2.\n",
      "Accessing details of item 5 on page 2...\n",
      "Data from item 5 scraped. Returning to the list...\n",
      "Successfully navigated to page 2.\n",
      "Accessing details of item 6 on page 2...\n",
      "Data from item 6 scraped. Returning to the list...\n",
      "Successfully navigated to page 2.\n",
      "Accessing details of item 7 on page 2...\n",
      "Data from item 7 scraped. Returning to the list...\n",
      "Successfully navigated to page 2.\n",
      "Accessing details of item 8 on page 2...\n",
      "Data from item 8 scraped. Returning to the list...\n",
      "Successfully navigated to page 2.\n",
      "Accessing details of item 9 on page 2...\n",
      "Data from item 9 scraped. Returning to the list...\n",
      "Successfully navigated to page 2.\n",
      "Accessing details of item 10 on page 2...\n",
      "Data from item 10 scraped. Returning to the list...\n",
      "Successfully navigated to page 2.\n",
      "Accessing details of item 11 on page 2...\n",
      "Data from item 11 scraped. Returning to the list...\n",
      "Successfully navigated to page 2.\n",
      "Accessing details of item 12 on page 2...\n",
      "Data from item 12 scraped. Returning to the list...\n",
      "Successfully navigated to page 2.\n",
      "Accessing details of item 13 on page 2...\n",
      "Data from item 13 scraped. Returning to the list...\n",
      "Successfully navigated to page 2.\n",
      "Accessing details of item 14 on page 2...\n",
      "Data from item 14 scraped. Returning to the list...\n",
      "Successfully navigated to page 2.\n",
      "Accessing details of item 15 on page 2...\n",
      "Data from item 15 scraped. Returning to the list...\n",
      "Successfully navigated to page 2.\n",
      "Navigating to page 3.\n",
      "Scraping data from page 3...\n",
      "Accessing details of item 1 on page 3...\n",
      "Data from item 1 scraped. Returning to the list...\n",
      "Successfully navigated to page 3.\n",
      "Accessing details of item 2 on page 3...\n",
      "Data from item 2 scraped. Returning to the list...\n",
      "Successfully navigated to page 3.\n",
      "Accessing details of item 3 on page 3...\n",
      "Data from item 3 scraped. Returning to the list...\n",
      "Successfully navigated to page 3.\n",
      "Accessing details of item 4 on page 3...\n",
      "Data from item 4 scraped. Returning to the list...\n",
      "Successfully navigated to page 3.\n",
      "Accessing details of item 5 on page 3...\n",
      "Data from item 5 scraped. Returning to the list...\n",
      "Successfully navigated to page 3.\n",
      "Accessing details of item 6 on page 3...\n",
      "Data from item 6 scraped. Returning to the list...\n",
      "Successfully navigated to page 3.\n",
      "Accessing details of item 7 on page 3...\n",
      "Data from item 7 scraped. Returning to the list...\n",
      "Successfully navigated to page 3.\n",
      "Accessing details of item 8 on page 3...\n",
      "Data from item 8 scraped. Returning to the list...\n",
      "Successfully navigated to page 3.\n",
      "Accessing details of item 9 on page 3...\n",
      "Data from item 9 scraped. Returning to the list...\n",
      "Successfully navigated to page 3.\n",
      "Accessing details of item 10 on page 3...\n",
      "Data from item 10 scraped. Returning to the list...\n",
      "Successfully navigated to page 3.\n",
      "Accessing details of item 11 on page 3...\n",
      "Data from item 11 scraped. Returning to the list...\n",
      "Successfully navigated to page 3.\n",
      "Accessing details of item 12 on page 3...\n",
      "Data from item 12 scraped. Returning to the list...\n",
      "Successfully navigated to page 3.\n",
      "Accessing details of item 13 on page 3...\n",
      "Data from item 13 scraped. Returning to the list...\n",
      "Successfully navigated to page 3.\n",
      "Accessing details of item 14 on page 3...\n",
      "Data from item 14 scraped. Returning to the list...\n",
      "Successfully navigated to page 3.\n",
      "Accessing details of item 15 on page 3...\n",
      "Data from item 15 scraped. Returning to the list...\n",
      "Successfully navigated to page 3.\n",
      "Navigating to page 4.\n",
      "Scraping data from page 4...\n",
      "Accessing details of item 1 on page 4...\n",
      "Data from item 1 scraped. Returning to the list...\n",
      "Successfully navigated to page 4.\n",
      "Accessing details of item 2 on page 4...\n",
      "Data from item 2 scraped. Returning to the list...\n",
      "Successfully navigated to page 4.\n",
      "Accessing details of item 3 on page 4...\n",
      "Data from item 3 scraped. Returning to the list...\n",
      "Successfully navigated to page 4.\n",
      "Accessing details of item 4 on page 4...\n",
      "Data from item 4 scraped. Returning to the list...\n",
      "Successfully navigated to page 4.\n",
      "Accessing details of item 5 on page 4...\n",
      "Data from item 5 scraped. Returning to the list...\n",
      "Successfully navigated to page 4.\n",
      "Accessing details of item 6 on page 4...\n",
      "Data from item 6 scraped. Returning to the list...\n",
      "Successfully navigated to page 4.\n",
      "Accessing details of item 7 on page 4...\n",
      "Data from item 7 scraped. Returning to the list...\n",
      "Successfully navigated to page 4.\n",
      "Accessing details of item 8 on page 4...\n",
      "Data from item 8 scraped. Returning to the list...\n",
      "Successfully navigated to page 4.\n",
      "Accessing details of item 9 on page 4...\n",
      "Data from item 9 scraped. Returning to the list...\n",
      "Successfully navigated to page 4.\n",
      "Accessing details of item 10 on page 4...\n",
      "Data from item 10 scraped. Returning to the list...\n",
      "Successfully navigated to page 4.\n",
      "Accessing details of item 11 on page 4...\n",
      "Data from item 11 scraped. Returning to the list...\n",
      "Successfully navigated to page 4.\n",
      "Accessing details of item 12 on page 4...\n",
      "Data from item 12 scraped. Returning to the list...\n",
      "Successfully navigated to page 4.\n",
      "Accessing details of item 13 on page 4...\n",
      "Data from item 13 scraped. Returning to the list...\n",
      "Successfully navigated to page 4.\n",
      "Accessing details of item 14 on page 4...\n",
      "Data from item 14 scraped. Returning to the list...\n",
      "Successfully navigated to page 4.\n",
      "Accessing details of item 15 on page 4...\n",
      "Data from item 15 scraped. Returning to the list...\n",
      "Successfully navigated to page 4.\n",
      "Navigating to page 5.\n",
      "Scraping data from page 5...\n",
      "Accessing details of item 1 on page 5...\n",
      "Data from item 1 scraped. Returning to the list...\n",
      "Successfully navigated to page 5.\n",
      "Accessing details of item 2 on page 5...\n",
      "Data from item 2 scraped. Returning to the list...\n",
      "Successfully navigated to page 5.\n",
      "Accessing details of item 3 on page 5...\n",
      "Data from item 3 scraped. Returning to the list...\n",
      "Successfully navigated to page 5.\n",
      "Accessing details of item 4 on page 5...\n",
      "Data from item 4 scraped. Returning to the list...\n",
      "No more detail items to scrape on page 5.\n",
      "No link found for page 6. Exiting loop.\n",
      "Closing the browser...\n",
      "Data scraping complete. File 'planning_history_01062021-01122021_applications_20240317_131229.csv' saved.\n"
     ]
    }
   ],
   "source": [
    "#https://www.casey.vic.gov.au/view-planning-applications\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "def get_value_by_label(soup, label_text):\n",
    "    try:\n",
    "        row = soup.find('td', text=label_text).find_parent('tr')\n",
    "        value_td = row.find_all('td')[1]\n",
    "        return value_td.text.strip()\n",
    "    except:\n",
    "        return \"Not Found\"\n",
    "\n",
    "def go_to_page(driver, page_number):\n",
    "    current_page = 1  # Assuming we start on the first page\n",
    "    \n",
    "    while current_page != page_number:\n",
    "        if current_page % 10 == 0 and page_number > current_page:\n",
    "            # Click the '...' link after every 10 pages to get to the next set of pages\n",
    "            ellipsis_xpath = '//*[@id=\"ctl00_Content_cusResultsGrid_repWebGrid_ctl00_grdWebGridTabularView\"]/tbody/tr[17]/td/table/tbody/tr/td[12]/a'\n",
    "            try:\n",
    "                driver.find_element(By.XPATH, ellipsis_xpath).click()\n",
    "                time.sleep(3)  # Wait for the next set of pages to load\n",
    "                current_page += 1  # Move to the first page of the next set\n",
    "            except NoSuchElementException:\n",
    "                print(\"Failed to find the '...' link for the next set of pages.\")\n",
    "                break\n",
    "        else:\n",
    "            if current_page < page_number:\n",
    "                next_page_number = current_page + 1\n",
    "                next_page_xpath = f'//*[@id=\"ctl00_Content_cusResultsGrid_repWebGrid_ctl00_grdWebGridTabularView\"]/tbody/tr[17]/td/table/tbody/tr/td/a[contains(text(), \"{next_page_number}\")]'\n",
    "                try:\n",
    "                    driver.find_element(By.XPATH, next_page_xpath).click()\n",
    "                    time.sleep(3)  # Wait for the next page to load\n",
    "                    current_page = next_page_number\n",
    "                except NoSuchElementException:\n",
    "                    print(f\"Failed to navigate to page {next_page_number}.\")\n",
    "                    break\n",
    "            elif current_page > page_number:\n",
    "                # Handle case if you need to navigate backwards (this part is not covered by your initial logic and might need custom implementation)\n",
    "                print(\"Navigating backwards is not implemented.\")\n",
    "                break\n",
    "    \n",
    "    if current_page == page_number:\n",
    "        print(f\"Successfully navigated to page {page_number}.\")\n",
    "    else:\n",
    "        print(f\"Failed to navigate to the desired page {page_number}.\")\n",
    "\n",
    "\n",
    "print(\"Setting up the Selenium WebDriver...\")\n",
    "driver = webdriver.Chrome()\n",
    "wait = WebDriverWait(driver, 10)\n",
    "\n",
    "print(\"Navigating to the initial URL...\")\n",
    "url = \"https://www.casey.vic.gov.au/view-planning-applications\"\n",
    "driver.get(url)\n",
    "\n",
    "# Input date values\n",
    "date_from_input = \"01/06/2021\"\n",
    "date_to_input = \"01/12/2021\"\n",
    "\n",
    "print(\"Clicking the necessary button to get to the search page...\")\n",
    "button_xpath = '//*[@id=\"block-content\"]/article/div/div/div/p[8]/a'\n",
    "button = wait.until(EC.element_to_be_clickable((By.XPATH, button_xpath)))\n",
    "button.click()\n",
    "\n",
    "print(\"Entering the desired date range...\")\n",
    "date_from_xpath = '//*[@id=\"ctl00_Content_txtDateFrom_txtText\"]'\n",
    "date_to_xpath = '//*[@id=\"ctl00_Content_txtDateTo_txtText\"]'\n",
    "date_from = wait.until(EC.element_to_be_clickable((By.XPATH, date_from_xpath)))\n",
    "date_to = wait.until(EC.element_to_be_clickable((By.XPATH, date_to_xpath)))\n",
    "\n",
    "date_from.clear()\n",
    "date_from.send_keys(date_from_input)\n",
    "date_to.clear()\n",
    "date_to.send_keys(date_to_input)\n",
    "\n",
    "\n",
    "print(\"Clicking the search button...\")\n",
    "search_button_xpath = '//*[@id=\"ctl00_Content_btnSearch\"]'\n",
    "search_button = wait.until(EC.element_to_be_clickable((By.XPATH, search_button_xpath)))\n",
    "search_button.click()\n",
    "\n",
    "print(\"Starting to scrape data...\")\n",
    "results = []\n",
    "detailed_results = []\n",
    "current_page = 1\n",
    "stored_current_page = 1\n",
    "current_index = 1\n",
    "\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        print(f\"Scraping data from page {current_page}...\")\n",
    "        wait.until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"ctl00_Content_cusResultsGrid_repWebGrid_ctl00_grdWebGridTabularView\"]')))\n",
    "        rows = driver.find_elements(By.XPATH, '//*[@id=\"ctl00_Content_cusResultsGrid_repWebGrid_ctl00_grdWebGridTabularView\"]/tbody/tr')\n",
    "        \n",
    "        for row in rows[1:-1]:  # Skipping the header and pagination rows\n",
    "            cells = row.find_elements(By.TAG_NAME, \"td\")\n",
    "            if len(cells) >= 7:  # Ensure there are enough cells\n",
    "                link = cells[0].find_element(By.TAG_NAME, \"a\").text if cells[0].find_elements(By.TAG_NAME, \"a\") else 'No Link'\n",
    "                date = cells[1].text\n",
    "                proposal = cells[2].text\n",
    "                app_type = cells[3].text\n",
    "                category_description = cells[4].text  # Assuming it's the fifth column\n",
    "                address = cells[5].text\n",
    "                status = cells[6].text\n",
    "                results.append([link, date, proposal, app_type, category_description, address, status])\n",
    "        \n",
    "        # Reset the index if it's a new page\n",
    "        if current_page != stored_current_page:\n",
    "            current_index = 1\n",
    "            \n",
    "             # Scrape detailed data\n",
    "        for row_index in range(1, len(rows) - 1):  # Adjust for the actual number of rows\n",
    "            try:\n",
    "                # Access details of the current row\n",
    "                details_link_selector = f'#ctl00_Content_cusResultsGrid_repWebGrid_ctl00_grdWebGridTabularView > tbody > tr:nth-child({row_index + 1}) > td:nth-child(1) > a'\n",
    "                driver.find_element(By.CSS_SELECTOR, details_link_selector).click()\n",
    "                print(f\"Accessing details of item {row_index} on page {current_page}...\")\n",
    "                time.sleep(5)\n",
    "\n",
    "                soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "                labels = [\"Application Number\", \"Application Type\", \"Estate Name\", \n",
    "                          \"Proposal Description\", \"Lodgement Date\",\n",
    "                          \"Estimated Value\", \"Status\", \"Further Info Requested Date\",\n",
    "                          \"Further Info Received Date\", \"No of Objections\", \"Decision\",\n",
    "                          \"Decision Date\", \"VCAT Lodged Date\", \"Public Open Space Exempt\",\n",
    "                          \"Correction\", \"PS Number\", \"PS Stage Number\", \"External Referral Exempt\",\n",
    "                          \"Public Open Space Exempt\", \"Titles Office Approval\", \"Final Outcome\",\n",
    "                          \"Final Outcome Date\", \"Property Address\", \"Land Description\",\n",
    "                          \"Ward\", \"eTrack Application Details Page\", \"Proposal\", \n",
    "                          \"Permit Type\", \"Relationship\", \"Advertising Commencement\", \"Advertising Completion\", \n",
    "                          \"Responsible Authority Outcome\", \"Version Lodged Date\", \"Permit Ext Start Date\", \n",
    "                          \"Permit Ext End Date\", \"Refer to TRANSPORT FOR VICTORIA (FORMERLY PTV)\",\n",
    "                          \"Refer to MELBOURNE WATER\",  \n",
    "                          \"Refer to CFA\", \"Refer to TRANSPORT FOR VICTORIA (VICROADS)\", \n",
    "                          \"Refer to APA VTS AUSTRALIA\", \"Refer to VICTRACK\", \"Refer to MELBOURNE WATER\", \n",
    "                          \"Refer to TRANSPORT FOR VICTORIA (FORMERLY PTV)\", \"Change Permit Applicant\", \n",
    "                          \"Section 50 Amendment\", \"Submit Additional Information\", \n",
    "                          \"Withdraw Application\", \"Respond to / Extend RFI\"]\n",
    "                data = {label: get_value_by_label(soup, label) for label in labels}\n",
    "                detailed_results.append(data)\n",
    "                \n",
    "                print(f\"Data from item {row_index} scraped. Returning to the list...\")\n",
    "                \n",
    "                \n",
    "                \n",
    "                 # Use the \"Previous\" button to return to the main list\n",
    "                return_button_xpath = '//*[@id=\"ctl00_Content_btnPrevious\"]'\n",
    "                driver.find_element(By.XPATH, return_button_xpath).click()\n",
    "                time.sleep(10)  # Wait for the main list to load\n",
    "\n",
    "#                 # Go back to the previous list and navigate to the correct page if necessary\n",
    "#                 driver.back()\n",
    "#                 time.sleep(5)  # Adjust sleep time as needed\n",
    "                \n",
    "                # Ensure you are on the correct page after returning from the details view\n",
    "                if current_page > 1:\n",
    "                    go_to_page(driver, current_page)  # You might need to adjust this function to work correctly if it's not currently\n",
    "                    wait.until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"ctl00_Content_cusResultsGrid_repWebGrid_ctl00_grdWebGridTabularView\"]')))\n",
    "                    time.sleep(2)  # Adjust timing as necessary\n",
    "                    \n",
    "\n",
    "            except NoSuchElementException:\n",
    "                print(f\"No more detail items to scrape on page {current_page}.\")\n",
    "                break\n",
    "\n",
    "        # Handle pagination\n",
    "        next_page_number = current_page + 1\n",
    "        if current_page % 10 == 0:\n",
    "            print(f\"Handling special pagination at page {current_page}...\")\n",
    "            ellipsis_xpath = f'//*[@id=\"ctl00_Content_cusResultsGrid_repWebGrid_ctl00_grdWebGridTabularView\"]/tbody/tr[17]/td/table/tbody/tr/td[{11 if current_page == 10 else 12}]/a'\n",
    "            try:\n",
    "                driver.find_element(By.XPATH, ellipsis_xpath).click()\n",
    "                print(f\"Clicked on '...' to navigate to special page after page {current_page}.\")\n",
    "                time.sleep(3)  # Wait for the next set of pages to load\n",
    "            except NoSuchElementException:\n",
    "                print(\"No '...' link found. Exiting loop.\")\n",
    "                break  # Exit loop if '...' link is not found\n",
    "        else:\n",
    "            next_page_xpath = f'//*[@id=\"ctl00_Content_cusResultsGrid_repWebGrid_ctl00_grdWebGridTabularView\"]/tbody/tr[17]/td/table/tbody/tr/td/a[contains(text(), \"{next_page_number}\")]'\n",
    "            try:\n",
    "                driver.find_element(By.XPATH, next_page_xpath).click()\n",
    "                print(f\"Navigating to page {next_page_number}.\")\n",
    "                time.sleep(5)  # Wait for the next page to load\n",
    "            except NoSuchElementException:\n",
    "                print(f\"No link found for page {next_page_number}. Exiting loop.\")\n",
    "                break  # Exit loop if next page number is not found\n",
    "\n",
    "        current_page += 1\n",
    "        stored_current_page = current_page\n",
    "\n",
    "    except TimeoutException:\n",
    "        print(\"Timeout exception occurred. Exiting loop.\")\n",
    "        break  # Exit the loop if the table is not visible\n",
    "\n",
    "print(\"Closing the browser...\")\n",
    "driver.quit()        \n",
    "\n",
    "\n",
    "# Create the first dataframe\n",
    "df = pd.DataFrame(results, columns=['App Link', 'Lodgement Date', 'Proposal', 'App Type', 'category_description', 'Address', 'Status'])\n",
    "\n",
    "# Create the second dataframe\n",
    "df_detailed = pd.DataFrame(detailed_results)\n",
    "\n",
    "# Concatenate the dataframes side by side\n",
    "combined_df = pd.concat([df, df_detailed], axis=1)\n",
    "\n",
    "# Get current datetime and format it as a string: YYYYMMDD_HHMMSS\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "# Format the output filename with the input dates\n",
    "date_from_formatted = date_from_input.replace(\"/\", \"\")\n",
    "date_to_formatted = date_to_input.replace(\"/\", \"\")\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "filename = f'planning_history_{date_from_formatted}-{date_to_formatted}_applications_{timestamp}.csv'\n",
    "combined_df.to_csv(filename, index=False)\n",
    "\n",
    "print(f\"Data scraping complete. File '{filename}' saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cb3361",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af92530d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58678e24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87446a1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7b37ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c3399c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78006ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ee4ee9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75912298",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139191f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21b971c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94401f81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a362e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a524057",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a1ab97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce509e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908a9a74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a160814b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a887a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446a1423",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e462dea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#backup code\n",
    "#https://www.casey.vic.gov.au/view-planning-applications\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "def get_value_by_label(soup, label_text):\n",
    "    try:\n",
    "        row = soup.find('td', text=label_text).find_parent('tr')\n",
    "        value_td = row.find_all('td')[1]\n",
    "        return value_td.text.strip()\n",
    "    except:\n",
    "        return \"Not Found\"\n",
    "\n",
    "def go_to_page(driver, page_number):\n",
    "    current_page = 1  # Assuming we start on the first page\n",
    "    \n",
    "    while current_page != page_number:\n",
    "        if current_page % 10 == 0 and page_number > current_page:\n",
    "            # Click the '...' link after every 10 pages to get to the next set of pages\n",
    "            ellipsis_xpath = '//*[@id=\"ctl00_Content_cusResultsGrid_repWebGrid_ctl00_grdWebGridTabularView\"]/tbody/tr[17]/td/table/tbody/tr/td[12]/a'\n",
    "            try:\n",
    "                driver.find_element(By.XPATH, ellipsis_xpath).click()\n",
    "                time.sleep(3)  # Wait for the next set of pages to load\n",
    "                current_page += 1  # Move to the first page of the next set\n",
    "            except NoSuchElementException:\n",
    "                print(\"Failed to find the '...' link for the next set of pages.\")\n",
    "                break\n",
    "        else:\n",
    "            if current_page < page_number:\n",
    "                next_page_number = current_page + 1\n",
    "                next_page_xpath = f'//*[@id=\"ctl00_Content_cusResultsGrid_repWebGrid_ctl00_grdWebGridTabularView\"]/tbody/tr[17]/td/table/tbody/tr/td/a[contains(text(), \"{next_page_number}\")]'\n",
    "                try:\n",
    "                    driver.find_element(By.XPATH, next_page_xpath).click()\n",
    "                    time.sleep(3)  # Wait for the next page to load\n",
    "                    current_page = next_page_number\n",
    "                except NoSuchElementException:\n",
    "                    print(f\"Failed to navigate to page {next_page_number}.\")\n",
    "                    break\n",
    "            elif current_page > page_number:\n",
    "                # Handle case if you need to navigate backwards (this part is not covered by your initial logic and might need custom implementation)\n",
    "                print(\"Navigating backwards is not implemented.\")\n",
    "                break\n",
    "    \n",
    "    if current_page == page_number:\n",
    "        print(f\"Successfully navigated to page {page_number}.\")\n",
    "    else:\n",
    "        print(f\"Failed to navigate to the desired page {page_number}.\")\n",
    "\n",
    "\n",
    "print(\"Setting up the Selenium WebDriver...\")\n",
    "driver = webdriver.Chrome()\n",
    "wait = WebDriverWait(driver, 10)\n",
    "\n",
    "print(\"Navigating to the initial URL...\")\n",
    "url = \"https://www.casey.vic.gov.au/view-planning-applications\"\n",
    "driver.get(url)\n",
    "\n",
    "# Input date values\n",
    "date_from_input = \"01/01/2021\"\n",
    "date_to_input = \"01/06/2021\"\n",
    "\n",
    "print(\"Clicking the necessary button to get to the search page...\")\n",
    "button_xpath = '//*[@id=\"block-content\"]/article/div/div/div/p[8]/a'\n",
    "button = wait.until(EC.element_to_be_clickable((By.XPATH, button_xpath)))\n",
    "button.click()\n",
    "\n",
    "print(\"Entering the desired date range...\")\n",
    "date_from_xpath = '//*[@id=\"ctl00_Content_txtDateFrom_txtText\"]'\n",
    "date_to_xpath = '//*[@id=\"ctl00_Content_txtDateTo_txtText\"]'\n",
    "date_from = wait.until(EC.element_to_be_clickable((By.XPATH, date_from_xpath)))\n",
    "date_to = wait.until(EC.element_to_be_clickable((By.XPATH, date_to_xpath)))\n",
    "\n",
    "date_from.clear()\n",
    "date_from.send_keys(date_from_input)\n",
    "date_to.clear()\n",
    "date_to.send_keys(date_to_input)\n",
    "\n",
    "\n",
    "print(\"Clicking the search button...\")\n",
    "search_button_xpath = '//*[@id=\"ctl00_Content_btnSearch\"]'\n",
    "search_button = wait.until(EC.element_to_be_clickable((By.XPATH, search_button_xpath)))\n",
    "search_button.click()\n",
    "\n",
    "print(\"Starting to scrape data...\")\n",
    "results = []\n",
    "detailed_results = []\n",
    "current_page = 1\n",
    "stored_current_page = 1\n",
    "current_index = 1\n",
    "\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        print(f\"Scraping data from page {current_page}...\")\n",
    "        wait.until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"ctl00_Content_cusResultsGrid_repWebGrid_ctl00_grdWebGridTabularView\"]')))\n",
    "        rows = driver.find_elements(By.XPATH, '//*[@id=\"ctl00_Content_cusResultsGrid_repWebGrid_ctl00_grdWebGridTabularView\"]/tbody/tr')\n",
    "        \n",
    "        for row in rows[1:-1]:  # Skipping the header and pagination rows\n",
    "            cells = row.find_elements(By.TAG_NAME, \"td\")\n",
    "            if len(cells) >= 7:  # Ensure there are enough cells\n",
    "                link = cells[0].find_element(By.TAG_NAME, \"a\").text if cells[0].find_elements(By.TAG_NAME, \"a\") else 'No Link'\n",
    "                date = cells[1].text\n",
    "                proposal = cells[2].text\n",
    "                app_type = cells[3].text\n",
    "                category_description = cells[4].text  # Assuming it's the fifth column\n",
    "                address = cells[5].text\n",
    "                status = cells[6].text\n",
    "                results.append([link, date, proposal, app_type, category_description, address, status])\n",
    "        \n",
    "        # Reset the index if it's a new page\n",
    "        if current_page != stored_current_page:\n",
    "            current_index = 1\n",
    "            \n",
    "             # Scrape detailed data\n",
    "        for row_index in range(1, len(rows) - 1):  # Adjust for the actual number of rows\n",
    "            try:\n",
    "                # Access details of the current row\n",
    "                details_link_selector = f'#ctl00_Content_cusResultsGrid_repWebGrid_ctl00_grdWebGridTabularView > tbody > tr:nth-child({row_index + 1}) > td:nth-child(1) > a'\n",
    "                driver.find_element(By.CSS_SELECTOR, details_link_selector).click()\n",
    "                print(f\"Accessing details of item {row_index} on page {current_page}...\")\n",
    "                time.sleep(5)\n",
    "\n",
    "                soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "                labels = [\"Application Number\", \"Application Type\", \"Estate Name\", \n",
    "                          \"Proposal Description\", \"Lodgement Date\",\n",
    "                          \"Estimated Value\", \"Status\", \"Further Info Requested Date\",\n",
    "                          \"Further Info Received Date\", \"No of Objections\", \"Decision\",\n",
    "                          \"Decision Date\", \"VCAT Lodged Date\", \"Public Open Space Exempt\",\n",
    "                          \"Correction\", \"PS Number\", \"PS Stage Number\", \"External Referral Exempt\",\n",
    "                          \"Public Open Space Exempt\", \"Titles Office Approval\", \"Final Outcome\",\n",
    "                          \"Final Outcome Date\", \"Property Address\", \"Land Description\",\n",
    "                          \"Ward\", \"eTrack Application Details Page\", \"Proposal\", \n",
    "                          \"Permit Type\", \"Relationship\", \"Advertising Commencement\", \"Advertising Completion\", \n",
    "                          \"Responsible Authority Outcome\", \"Version Lodged Date\", \"Permit Ext Start Date\", \n",
    "                          \"Permit Ext End Date\", \"Refer to TRANSPORT FOR VICTORIA (FORMERLY PTV)\",\n",
    "                          \"Refer to MELBOURNE WATER\",  \n",
    "                          \"Refer to CFA\", \"Refer to TRANSPORT FOR VICTORIA (VICROADS)\", \n",
    "                          \"Refer to APA VTS AUSTRALIA\", \"Refer to VICTRACK\", \"Refer to MELBOURNE WATER\", \n",
    "                          \"Refer to TRANSPORT FOR VICTORIA (FORMERLY PTV)\", \"Change Permit Applicant\", \n",
    "                          \"Section 50 Amendment\", \"Submit Additional Information\", \n",
    "                          \"Withdraw Application\", \"Respond to / Extend RFI\"]\n",
    "                data = {label: get_value_by_label(soup, label) for label in labels}\n",
    "                detailed_results.append(data)\n",
    "                \n",
    "                print(f\"Data from item {row_index} scraped. Returning to the list...\")\n",
    "                \n",
    "                \n",
    "                \n",
    "                 # Use the \"Previous\" button to return to the main list\n",
    "                return_button_xpath = '//*[@id=\"ctl00_Content_btnPrevious\"]'\n",
    "                driver.find_element(By.XPATH, return_button_xpath).click()\n",
    "                time.sleep(10)  # Wait for the main list to load\n",
    "\n",
    "#                 # Go back to the previous list and navigate to the correct page if necessary\n",
    "#                 driver.back()\n",
    "#                 time.sleep(5)  # Adjust sleep time as needed\n",
    "                \n",
    "                # Ensure you are on the correct page after returning from the details view\n",
    "                if current_page > 1:\n",
    "                    go_to_page(driver, current_page)  # You might need to adjust this function to work correctly if it's not currently\n",
    "                    wait.until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"ctl00_Content_cusResultsGrid_repWebGrid_ctl00_grdWebGridTabularView\"]')))\n",
    "                    time.sleep(2)  # Adjust timing as necessary\n",
    "                    \n",
    "\n",
    "            except NoSuchElementException:\n",
    "                print(f\"No more detail items to scrape on page {current_page}.\")\n",
    "                break\n",
    "\n",
    "        # Handle pagination\n",
    "        next_page_number = current_page + 1\n",
    "        if current_page % 10 == 0:\n",
    "            print(f\"Handling special pagination at page {current_page}...\")\n",
    "            ellipsis_xpath = f'//*[@id=\"ctl00_Content_cusResultsGrid_repWebGrid_ctl00_grdWebGridTabularView\"]/tbody/tr[17]/td/table/tbody/tr/td[{11 if current_page == 10 else 12}]/a'\n",
    "            try:\n",
    "                driver.find_element(By.XPATH, ellipsis_xpath).click()\n",
    "                print(f\"Clicked on '...' to navigate to special page after page {current_page}.\")\n",
    "                time.sleep(3)  # Wait for the next set of pages to load\n",
    "            except NoSuchElementException:\n",
    "                print(\"No '...' link found. Exiting loop.\")\n",
    "                break  # Exit loop if '...' link is not found\n",
    "        else:\n",
    "            next_page_xpath = f'//*[@id=\"ctl00_Content_cusResultsGrid_repWebGrid_ctl00_grdWebGridTabularView\"]/tbody/tr[17]/td/table/tbody/tr/td/a[contains(text(), \"{next_page_number}\")]'\n",
    "            try:\n",
    "                driver.find_element(By.XPATH, next_page_xpath).click()\n",
    "                print(f\"Navigating to page {next_page_number}.\")\n",
    "                time.sleep(5)  # Wait for the next page to load\n",
    "            except NoSuchElementException:\n",
    "                print(f\"No link found for page {next_page_number}. Exiting loop.\")\n",
    "                break  # Exit loop if next page number is not found\n",
    "\n",
    "        current_page += 1\n",
    "        stored_current_page = current_page\n",
    "\n",
    "    except TimeoutException:\n",
    "        print(\"Timeout exception occurred. Exiting loop.\")\n",
    "        break  # Exit the loop if the table is not visible\n",
    "\n",
    "print(\"Closing the browser...\")\n",
    "driver.quit()        \n",
    "\n",
    "\n",
    "# Create the first dataframe\n",
    "df = pd.DataFrame(results, columns=['App Link', 'Lodgement Date', 'Proposal', 'App Type', 'category_description', 'Address', 'Status'])\n",
    "\n",
    "# Create the second dataframe\n",
    "df_detailed = pd.DataFrame(detailed_results)\n",
    "\n",
    "# Concatenate the dataframes side by side\n",
    "combined_df = pd.concat([df, df_detailed], axis=1)\n",
    "\n",
    "# Get current datetime and format it as a string: YYYYMMDD_HHMMSS\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "# Format the output filename with the input dates\n",
    "date_from_formatted = date_from_input.replace(\"/\", \"\")\n",
    "date_to_formatted = date_to_input.replace(\"/\", \"\")\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "filename = f'planning_history_{date_from_formatted}-{date_to_formatted}_applications_{timestamp}.csv'\n",
    "combined_df.to_csv(filename, index=False)\n",
    "\n",
    "print(f\"Data scraping complete. File '{filename}' saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
